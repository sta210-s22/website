{
  "hash": "1d6717aed824f92a292d54b88a2c72bc",
  "result": {
    "markdown": "---\ntitle: \"Cross validation\"\nsubtitle: \"STA 210 - Spring 2022\"\nauthor: \"Dr. Mine Ã‡etinkaya-Rundel\"\nfooter: \"[sta210-s22.github.io/website](https://sta210-s22.github.io/website/)\"\nlogo: \"images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: true\n    transition: fade\n    slide-number: true\n    incremental: true \n    chalkboard: true\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\n---\n\n\n\n# Welcome\n\n## Topics\n\n::: nonincremental\n-   Cross validation for model evaluation\n-   Cross validation for model comparison\n:::\n\n## Computational setup\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(schrute)\n```\n:::\n\n## Data & goal {.smaller}\n\n::: nonincremental\n-   Data: The data come from the [**shrute**](https://bradlindblad.github.io/schrute/) package, and has been transformed using instructions from [Lab 4](/labs/lab-4.html)\n-   Goal: Predict `imdb_rating` from other variables in the dataset\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noffice_episodes <- read_csv(here::here(\"slides\", \"data/office_episodes.csv\"))\noffice_episodes\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 186 Ã— 14\n   season episode episode_name      imdb_rating total_votes air_date   lines_jim\n    <dbl>   <dbl> <chr>                   <dbl>       <dbl> <date>         <dbl>\n 1      1       1 Pilot                     7.6        3706 2005-03-24    0.157 \n 2      1       2 Diversity Day             8.3        3566 2005-03-29    0.123 \n 3      1       3 Health Care               7.9        2983 2005-04-05    0.172 \n 4      1       4 The Alliance              8.1        2886 2005-04-12    0.202 \n 5      1       5 Basketball                8.4        3179 2005-04-19    0.0913\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26    0.159 \n 7      2       1 The Dundies               8.7        3213 2005-09-20    0.125 \n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27    0.0565\n 9      2       3 Office Olympics           8.4        2742 2005-10-04    0.196 \n10      2       4 The Fire                  8.4        2713 2005-10-11    0.160 \n# â€¦ with 176 more rows, and 7 more variables: lines_pam <dbl>,\n#   lines_michael <dbl>, lines_dwight <dbl>, halloween <chr>, valentine <chr>,\n#   christmas <chr>, michael <chr>\n```\n:::\n:::\n\n# Modeling prep\n\n## Split data into training and testing\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\noffice_split <- initial_split(office_episodes)\noffice_train <- training(office_split)\noffice_test <- testing(office_split)\n```\n:::\n\n## Specify model\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noffice_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\noffice_spec\n```\n\n::: {.cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n# Model 1\n\n## From yesterday's lab\n\n-   Create a recipe that uses the new variables we generated\n-   Denotes `episode_name` as an ID variable and doesn't use `air_date` as a predictor\n-   Create dummy variables for all nominal predictors\n-   Remove all zero variance predictors\n\n## Create recipe\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noffice_rec1 <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(episode_name, new_role = \"id\") %>%\n  step_rm(air_date) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\noffice_rec1\n```\n\n::: {.cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor         12\n\nOperations:\n\nDelete terms air_date\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()\n```\n:::\n:::\n\n## Create workflow\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noffice_wflow1 <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec1)\n\noffice_wflow1\n```\n\n::: {.cell-output-stdout}\n```\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: linear_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n3 Recipe Steps\n\nâ€¢ step_rm()\nâ€¢ step_dummy()\nâ€¢ step_zv()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n## Fit model to training data\n\n. . .\n\n*Actually, not so fast!*\n\n# Cross validation\n\n## Spending our data\n\n-   We have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.\n-   However, we usually need to understand the effectiveness of the model *before using the test set*.\n-   Typically we can't decide on *which* final model to take to the test set without making model assessments.\n-   Remedy: Resampling to make model assessments on training data in a way that can generalize to new data.\n\n## Resampling for model assessment\n\n**Resampling is only conducted on the training set**.\nThe test set is not involved.\nFor each iteration of resampling, the data are partitioned into two subsamples:\n\n-   The model is fit with the **analysis set**.\n-   The model is evaluated with the **assessment set**.\n\n## Resampling for model assessment\n\n![](images/lec-14/resampling.svg){fig-align=\"center\"}\n\n<br>\n\nSource: Kuhn and Silge.\n[Tidy modeling with R](https://www.tmwr.org/).\n\n## Analysis and assessment sets\n\n-   Analysis set is analogous to training set.\n-   Assessment set is analogous to test set.\n-   The terms *analysis* and *assessment* avoids confusion with initial split of the data.\n-   These data sets are mutually exclusive.\n\n## Cross validation\n\nMore specifically, **v-fold cross validation** -- commonly used resampling technique:\n\n-   Randomly split your **training** **data** into v partitions\n-   Use 1 partition for assessment, and the remaining v-1 partitions for analysis\n-   Repeat v times, updating which partition is used for assessment each time\n\n. . .\n\nLet's give an example where `v = 3`...\n\n## Cross validation, step 1\n\nRandomly split your **training** **data** into 3 partitions:\n\n<br>\n\n![](images/lec-14/three-CV.svg){fig-align=\"center\"}\n\n## Split data\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(345)\nfolds <- vfold_cv(office_train, v = 3)\nfolds\n```\n\n::: {.cell-output-stdout}\n```\n#  3-fold cross-validation \n# A tibble: 3 Ã— 2\n  splits          id   \n  <list>          <chr>\n1 <split [92/47]> Fold1\n2 <split [93/46]> Fold2\n3 <split [93/46]> Fold3\n```\n:::\n:::\n\n## Cross validation, steps 2 and 3\n\n::: nonincremental\n-   Use 1 partition for assessment, and the remaining v-1 partitions for analysis\n-   Repeat v times, updating which partition is used for assessment each time\n:::\n\n![](images/lec-14/three-CV-iter.svg){fig-align=\"center\"}\n\n## Fit resamples\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(456)\n\noffice_fit_rs1 <- office_wflow1 %>%\n  fit_resamples(folds)\n\noffice_fit_rs1\n```\n\n::: {.cell-output-stdout}\n```\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 Ã— 4\n  splits          id    .metrics         .notes          \n  <list>          <chr> <list>           <list>          \n1 <split [92/47]> Fold1 <tibble [2 Ã— 4]> <tibble [0 Ã— 1]>\n2 <split [93/46]> Fold2 <tibble [2 Ã— 4]> <tibble [0 Ã— 1]>\n3 <split [93/46]> Fold3 <tibble [2 Ã— 4]> <tibble [0 Ã— 1]>\n```\n:::\n:::\n\n## Cross validation, now what?\n\n-   We've fit a bunch of models\n-   Now it's time to use them to collect metrics (e.g., R-squared, RMSE) on each model and use them to evaluate model fit and how it varies across folds\n\n## Collect CV metrics\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(office_fit_rs1)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 2 Ã— 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.351     3  0.0111 Preprocessor1_Model1\n2 rsq     standard   0.546     3  0.0378 Preprocessor1_Model1\n```\n:::\n:::\n\n## Deeper look into CV metrics\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_metrics1 <- collect_metrics(office_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 Ã— 5\n  id    .metric .estimator .estimate .config             \n  <chr> <chr>   <chr>          <dbl> <chr>               \n1 Fold1 rmse    standard       0.356 Preprocessor1_Model1\n2 Fold1 rsq     standard       0.520 Preprocessor1_Model1\n3 Fold2 rmse    standard       0.367 Preprocessor1_Model1\n4 Fold2 rsq     standard       0.498 Preprocessor1_Model1\n5 Fold3 rmse    standard       0.330 Preprocessor1_Model1\n6 Fold3 rsq     standard       0.621 Preprocessor1_Model1\n```\n:::\n:::\n\n## Better tabulation of CV metrics\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_metrics1 %>%\n  mutate(.estimate = round(.estimate, 3)) %>%\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) %>%\n  kable(col.names = c(\"Fold\", \"RMSE\", \"R-squared\"))\n```\n\n::: {.cell-output-display}\n|Fold  |  RMSE| R-squared|\n|:-----|-----:|---------:|\n|Fold1 | 0.356|     0.520|\n|Fold2 | 0.367|     0.498|\n|Fold3 | 0.330|     0.621|\n:::\n:::\n\n## How does RMSE compare to y? {.smaller}\n\nCross validation RMSE stats:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_metrics1 %>%\n  filter(.metric == \"rmse\") %>%\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 Ã— 4\n    min   max  mean     sd\n  <dbl> <dbl> <dbl>  <dbl>\n1 0.330 0.367 0.351 0.0192\n```\n:::\n:::\n\nTraining data IMDB score stats:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noffice_episodes %>%\n  summarise(\n    min = min(imdb_rating),\n    max = max(imdb_rating),\n    mean = mean(imdb_rating),\n    sd = sd(imdb_rating)\n  )\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 Ã— 4\n    min   max  mean    sd\n  <dbl> <dbl> <dbl> <dbl>\n1   6.7   9.7  8.25 0.535\n```\n:::\n:::\n\n## Cross validation jargon\n\n-   Referred to as v-fold or k-fold cross validation\n-   Also commonly abbreviated as CV\n\n## Cross validation, for reals\n\n-   To illustrate how CV works, we used `v = 3`:\n\n    ::: nonincremental\n    -   Analysis sets are 2/3 of the training set\n    -   Each assessment set is a distinct 1/3\n    -   The final resampling estimate of performance averages each of the 3 replicates\n    :::\n\n-   This was useful for illustrative purposes, but `v = 3` is a poor choice in practice\n\n-   Values of `v` are most often 5 or 10; we generally prefer 10-fold cross-validation as a default\n\n# Application exercise\n\n::: appex\nğŸ“‹ [github.com/sta210-s22/ae-6-the-office-cv](https://github.com/sta210-s22/ae-6-the-office-cv)\n:::\n\n## Recap\n\n::: nonincremental\n-   Cross validation for model evaluation\n-   Cross validation for model comparison\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}